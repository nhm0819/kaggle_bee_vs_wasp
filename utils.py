# -*- coding: utf-8 -*-
"""
Created on Thu May 20 22:49:59 2021

@author: PC
"""
from tensorflow import keras


class ResidualUnit(keras.layers.Layer):
    def __init__(self, filters, strides=1, activation="elu", **kwargs):
        super().__init__(**kwargs)
        self.activation = keras.activations.get(activation)
        self.main_layers = [
                            keras.layers.Conv2D(filters, 3, strides=strides,
                                                padding="same", use_bias=False),
                            keras.layers.BatchNormalization(),
                            self.activation,
                            keras.layers.Conv2D(filters, 3, strides=strides,
                                                padding="same", use_bias=False),
                            keras.layers.BatchNormalization()
        ]
        self.skip_layers = []
        if strides > 1:
            self.skip_layers = [
                                keras.layers.Conv2D(filters, 1, strides=strides,
                                                    padding="same", use_bias=False),
                                keras.layers.BatchNormalization()
            ]

        def call(self, inputs):
            Z = inputs
            for layer in self.main_layers:
                Z = layer(Z)
            skip_Z = inputs
            for layer in self.skip_layers:
                skip_Z = layer(skip_Z)
            return self.activation(Z + skip_Z)